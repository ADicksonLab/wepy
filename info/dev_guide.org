* Development


** Getting Set Up

*** Obtaining the source code

We do development on github:

#+BEGIN_SRC bash
git clone https://github.com/ADicksonLab/wepy --recurse-submodules
cd wepy
#+END_SRC

*** Tooling

To make things easier for developers we provide a set automation
scripts implemented in ~invoke~ (similar to a makefile if you are
familiar with that).

So you will want to install invoke somehow to use this tooling. The
easiest most contained way to do this we have found is to use ~pipx~:

#+begin_src bash
pipx install invoke
#+end_src


*** Virtual Environments

This is for managing environments which are just for the purpose of
developing wepy, and not necessarily just for running it (as a
user). See [[*Managing Dependencies][Managing Dependencies]] for details on managing dependencies
of the installable project.

To develop on ~wepy~ we require OpenMM. By far the easiest way to get
this is to use conda. Since we already dependent on anaconda for this
we currently also use anaconda virtual environments rather than the
standard library ones. It also makes a few things we depend on like
~pandoc~ easier to install for building docs.

We also require a few shell environmental variables which are exported
in the ~env.bash~ file. Go ahead and source this before doing anything
else:

#+begin_src bash
source env.bash
#+end_src

To create an env called ~wepy.dev~ just run the ~env~ target from
~invoke~:

#+begin_src bash
inv env
#+end_src

Then activate it:

#+begin_src bash
conda activate wepy.dev
#+end_src

If you ever have problems with an environment just rerun this to get a
clean one. A practice we encourage to do frequently so that developers
don't diverge in their envs with local modifications. So while you can
make your env, try to use this one unless you have problems.

We maintain a number of preconfigured environments in the ~envs~
directory which are used for different purposes. Calling ~inv env -n
dev~ is the same as ~inv dev~ since it is the default, but any other
environment can be created by passing the matching name. For instance
there is an environment that mimics the user's installation
environment so that we can test experiences upon install, to make sure
we haven't accidentally depended on something in the dev env. For
instance:

#+begin_src bash
inv env -n trial
conda activate wepy.trial
#+end_src

If you want to make another environment it is straightforward to copy
the examples in the ~envs~ dir.


** Tasks

*** Licenses and Copyright

The license can be found in the LICENSE file and shouldn't need to be
updated usually.

However, according to copyright laws (hand waves...) the original date
in the license establishes the earliest copyright claim to the
work. The date shouldn't need updated in principle. The only thing you
could do is add dates to this. Adding dates may be useful if
significant changes to the software are made.

So rule of thumbs are:

- never remove the original date in the license copyright
- you can add dates if you want when you make big changes, but it is
  probably not necessary


*** Managing Dependencies


Reminder that there are two separate goals of managing dependencies
and where they are managed:

- Python Libraries :: These dependencies are managed in ~setup.py~ and
  in PyPI or other indices.
- Python Applications/Deployments :: These are dependencies managed in
  ~requirements.in~/~requirements.txt~ and used for developer
  environments and deployment environments.

So for the library aspect we use abstract requirements. These should
essentially be the same as ~requirements.in~.

For the deployment side of things we use ~requirements.txt~. Don't
manually edit this. We use ~pip-tools~ to "compile" dependencies for
this.

# TODO: figure out high level and pinned conda version files

To initially pin an environment or when you add requirements run this
target:

#+begin_src bash
inv deps-pin
#+end_src

To update it (should be accompanied by a reason why):

#+begin_src bash
inv deps-pin-update
#+end_src

*** Documentation and Website

**** Editing and Building Docs

To compile and build the docs just run:

#+begin_src bash
inv docs-build
#+end_src

Which will output them to a temporary build directory ~_build/html~.

You can clean this build with:

#+begin_src bash
inv clean-docs
#+end_src


To view how the docs would look as a website you can point your
browser at the ~_build/html~ folder or run a python http web server
with this target:

#+begin_src bash
inv docs-serve
#+end_src


**** COMMENT TODO: WIP: Building and testing the website

The website is still a work in progress and is located in the ~jekyll~
folder.

The website uses jekyll and so you must have ~ruby~, ~bundler~, and
~jekyll~ installed.

On ubuntu and debian:

#+begin_src bash
sudo apt install -y ruby-full build-essential zlib1g-dev
#+end_src

And then on whichever distro with ~GEM_HOME~ on your ~PATH~:

#+begin_src bash
gem install jekyll bundler
#+end_src


Then you just need to run this command:

#+begin_src bash
inv website-deploy-local
#+end_src


**** Deploying the website

We are using github pages. To avoid having to keep the entire built
website in the main tree we use the alternate ~gh-pages~ branch. To
make this process easy to deploy we have a script ~sphinx/deploy.sh~
that checks the ~gh-pages~ branch out, does some necessary cleaning
up, and copies the built website to the necesary folder (which is the
toplevel), commits the changes and pushes to github, and then returns
to your working branch.

The invoke target is:

#+begin_src bash
inv website-deploy
#+end_src


*** Testing

**** Getting the wepy-tests submodule

The tests for wepy are included as a submodule because some of the
associated data is large and we want to make the install base for the
program smaller than that. Development of this is tracked in
https://gitlab.com/salotz/wepy-tests.

If you cloned without the recurse-submodules flag you can always pull
them in later like this:

#+begin_src bash
git submodule update --init --recursive
#+end_src


WARNING: before you start editing the ~wepy-tests~ submodule you need
to check out master.

#+begin_src bash
git checkout master
#+end_src

How many times I have edited it before I checked out master...

If you do edit and commit try to get the hash of the commit and then
merge with master. If you don't then you need to figure out which
commit that was.




*** Code Quality Metrics

Just run the end target:

#+begin_src bash
inv quality
#+end_src

This will write files to ~metrics~.

*** Releases


**** Choosing a version number

- semver :: major, minor, patch
- release candidates
- dev
- post release

**** Changing the version number

You can check the current version number with this command:

#+begin_src bash
inv version-which
#+end_src

Increase the version number we currently do it by hand (although an
automatic way would be nice and ~bumpversion~ wasn't working for
us). So go in and manually edit them. For reference see PEP 440 for
valid ones.

The targets are in the ~.bumpversion.cfg~ for reference, but at a high
level:

- [ ] ~setup.py~
- [ ] ~src/wepy/__init__.py~
- [ ] ~sphinx/config.py~
- [ ] ~.zenodo.json~
- [ ] ~conda/conda-forge/meta.yaml~







**** Writing and/or Generating the Changelog and Announcement
**** Making the release official

To make a release do some changes and make sure they are fully tested
and functional and commit them in version control. At this point you
will also want to do any rebasing or cleaning up the actual commits if
this wasn't already done in the feature branch.

If this is a 'dev' release and you just want to run a version control
tag triggered CI pipeline go ahead and change the version numbers and
commit. Then tag the 'dev' release.

If you intend to make a non-dev release you will first want to test it
out a little bit with a release-candidate prerelease.

So do all the following bookeeping steps in a single but separate git
commit from the actual changes to the code:

- [ ] write the changelog
- [ ] write the announcement (optional)
- [ ] change the version numbers
- [ ] build to test it out ~inv build~

However, when you change the version number put a 'rc0' at the end of
the new intended (semantic) number.

Once you have built it and nothing is wrong go ahead and publish it to
the test indexes (if available):

#+begin_src bash
inv publish-test
#+end_src

You can test that it works by installing from the ~base~ env.

Make it if you haven't:

#+begin_src bash
inv env -n base
#+end_src

Then activate it:

#+begin_src bash
source _venv/base/bin/activate
#+end_src

And install the package from the test repo with no dependencies:

#+begin_src bash
pip install --index-url https://test.pypi.org/simple/ --no-deps {{cookiecutter.project_name}}-{{cookiecutter.owner_nickname}}
#+end_src

# QUEST: should this message be the release message we want for the VCS
# repos to show or should we just point them to the changelog?

Then go ahead and commit the changes after that works. The message
should follow a fixed form like 

#+begin_src fundamental
1.0.0rc0 release preparation
#+end_src

Then you can tag the release in the ~git~ commit history:

#+begin_src bash
inv release
#+end_src

Publishing the results will vary but you can start with publishing the
package to PyPI and the VCS hosts with the real publish target:

#+begin_src bash
inv publish
#+end_src


* COMMENT OLD: Development Setup

Get the source code:

#+BEGIN_SRC bash
git clone https://github.com/ADicksonLab/wepy --recurse-submodules
cd wepy
#+END_SRC

Install a virtual environment for it:

#+BEGIN_SRC bash
  wepy_dev_env_refresh () {

      package='wepy'
      conda deactivate
      dev_env="${package}-dev"
      conda env remove -y -n "$dev_env"
      conda create -y -n "$dev_env" python=3
      conda activate "$dev_env"

      # we need openmm but can't get it from pip
      conda install -y -c omnia openmm openmmtools

      # install in editable mode, we need to avoid using pep517 which
      # doesn't allow editable installs
      pip install -r requirements_dev.txt 
      pip install --no-use-pep517 -e .[all]

  }
#+END_SRC

#+BEGIN_SRC bash
wepy_dev_env_refresh
#+END_SRC


Currently, for installing mdtraj we use a forked repository which
handles pip installations better that allows for seamless dependecy
resolution and doesn't require manual intervention to install cython.

This is specified in the requirements.txt file which should be used
for specifying the "concrete" requirements of the project (i.e. the
literal repo or index URL that packages should be retrieved from).

"Abstract" requirements should also be listed in setup.py.

For development specific requirements, we have the separate
requirements_dev.txt.

Because at this multiple packages are developed simultaneously we
require that geomm be installed in the same directory as wepy for
using the dev requirements.

** Releasing Package

*** Test the installation process

Functions for doing this:

#+BEGIN_SRC bash
  wepy_test_build () {
      package='wepy'
      build_env="test-${package}-build"
      conda deactivate
      conda env remove -y -n "$build_env"
      conda create -y -n "$build_env" python=3
      conda activate "$build_env"
      pip install -r requirements_dev.txt
      rm -rf dist/*
      python setup.py build sdist
      conda deactivate
      conda env remove -y -n "$build_env"

  }

  wepy_test_install () {

      package='wepy'
      conda deactivate
      install_env="test-${package}-install"
      conda env remove -y -n "$install_env"
      conda create -y -n "$install_env" python=3
      conda activate "$install_env"
      pip install dist/"$package"-*.tar.gz
      conda deactivate
      conda env remove -y -n "$install_env"

  }
#+END_SRC

*** Update versions


Before we build the package we need to bump the version in all those
places it is written down at, which is achieved with the bumpversion
tool:

#+BEGIN_SRC bash
bumpversion patch # possible: major / minor / patch
#+END_SRC

Make sure to tag in git (I typically use magit in emacs but the
command is):

#+BEGIN_SRC bash
git tag -a vX.Y.Z -m "release message"
git push gitlab vX.Y.Z
#+END_SRC

*** Deploying

To deploy to PyPI (if you have access)
#+BEGIN_SRC bash
conda activate wepy-dev
rm -rf dist/*
python setup.py sdist
twine upload dist/*
#+END_SRC



** Building Docs

Install pandoc for converting org-mode files to rst.

You can follow the instructions on the site or just use anaconda:

#+BEGIN_SRC bash
conda install pandoc
#+END_SRC

Then run the build script. This uses the make file and additionally
runs api-doc, and converts org-mode source files to rst using pandoc.

#+BEGIN_SRC bash
pushd sphinx
chmod u+x build.sh
./build.sh
popd
#+END_SRC

This will build the HTML files in the ~sphinx/_build/html~ directory
and if you point your web browser there you can view them.

** Deploying Docs

To run the current deployments of the docs run the deploy script:

#+BEGIN_SRC bash
pushd sphinx
chmod u+x deploy.sh
./deploy.sh
popd
#+END_SRC

Currently we are using github pages, and to avoid putting the build
artifacts of the website into the master development branch we are
using the gh-pages branch.

To make this work you need to pull the gh-pages branch:




** Testing

*** Getting the wepy-tests submodule

The tests for wepy are included as a submodule because some of the
associated data is large and we want to make the install base for the
program smaller than that. Development of this is tracked in
https://gitlab.com/salotz/wepy-tests.

If you cloned without the recurse-submodules flag you can always pull
them in later like this:

#+begin_src bash
git submodule update --init --recursive
#+end_src


WARNING: before you start editing the ~wepy-tests~ submodule you need
to check out master.

#+begin_src bash
git checkout master
#+end_src

How many times I have edited it before I checked out master...

If you do edit and commit try to get the hash of the commit and then
merge with master. If you don't then you need to figure out which
commit that was.

*** Test Suite
We are using pytest so just run that from the main directory:

#+BEGIN_SRC bash
pytest
#+END_SRC

We use a special marker for interacting with test fixtures. We find
this more useful in many cases where you just want to spin up a test
fixture with the newest changes and inspect it, perhaps to help in
writing real tests. We incorporate this with the testing suite so we
only have to implement the boilerplate code of setting up test
fixtures once, and we gain that it is now version controlled.

To select just the interactive tests (which just have
a fixture and a breakpoint) run:

#+BEGIN_SRC bash
pytest -m interactive
#+END_SRC

To run automated tests:

#+BEGIN_SRC bash
pytest -m 'not interactive'
#+END_SRC

TODO: we will probably add more categories in the future for selecting
particular fixtures.

We are also using tox to test against different python versions. To
test against all of the versions they must be installed on the machine
in a directory here called `PREFIX`. To let tox see them they must be
on your path so run tox with a modified environment so we don't have
to dingle with the path in an interactive shell and confuse ourselves:

#+BEGIN_SRC bash
env PATH="$PREFIX/bin:$PATH" tox
#+END_SRC

To install these different pythons download, unpack and build the
python configuring it to be installed to the prefix:

#+BEGIN_SRC bash
wget "https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz"
tar --extract -f Python-3.7.3
cd Python-3.7.3
./configure --prefix=$PREFIX
make -j 8
make install
#+END_SRC

To run tox for a specific environment check which environment names
are possible by looking in the `tox.ini` file:

#+BEGIN_SRC bash
env PATH="$PREFIX/bin:$PATH" tox -r -e py37
#+END_SRC

Where the `-r` option recreates it from scratch.


*** Code Quality

You can also lint the code with flake8:

#+BEGIN_SRC bash
flake8 src/wepy wepy-tests
#+END_SRC

And get reports on the complexity of our code:

TODO

*** Profiling

We also have tests for profiling the performance sensitive parts of
our code.

You will need to install graphviz for this to get nice SVGs of the
call graphs. On ubuntu and debian:

#+begin_src bash
  sudo apt install -y graphviz
#+end_src

*** Testing examples and tutorials

We also want to make sure that the tutorials and examples work.

For this we want to emulate the experience of somebody installing it
from scratch and running the examples.

#+BEGIN_SRC bash
  wepy_test_user_install () {

      package='wepy'
      conda deactivate
      install_env="test-${package}-user-install"
      conda env remove -y -n "$install_env"
      conda create -y -n "$install_env" python=3
      conda activate "$install_env"
      conda install -y -c omnia openmm openmmtools
      pip install wepy[all]==1.0.0rc0
  }

  wepy_test_user_master_install () {

      package='wepy_master'
      conda deactivate
      install_env="test-${package}-user-install"
      conda env remove -y -n "$install_env"
      conda create -y -n "$install_env" python=3
      conda activate "$install_env"
      conda install -y -c omnia openmm openmmtools
      pip install mdtraj
      pip install git+https://github.com/ADicksonLab/wepy.git
  }
#+END_SRC


**** Examples

*** Writing Tests

If you add a feature ideally you should add some sort of test to make
sure it works.

We currently don't do extensive tests at fine grained levels like unit
tests. Largely, I think these are a waste of time for a project like
wepy without a full time developer. These are welcome contributions
however, if anyone finds the time to write them.

Our tests do however try to do some basic integration tests where we
just try to build up and run simulations and perhaps run some analysis
routines just to make sure that your changes or new component can be
run without errors somewhere down the line.

Aside from the automated tests which get run by pytest there are a
number of other useful pieces of code that tend to be useful during
the development or perhaps maintenance cycle. This is a little
different from other repos I have seen, and perhaps adds a little bit
of messiness to the whole thing. It should add however, some value to
dealing with difficult and slippery problems that at least I have
encountered in the day to day of developing a project. Our goal is to
have clear boundaries for quarantining our messiness so that it
doesn't inevitably bleed into the perfectly crystalline purity of the
main code base. A complete lack of messiness (IMO) is either a sign of
immense maturity (unlikely) or premature optimization. So we aim to
start treating it as a first class citizen.

These categories and the related folders are:

- tests :: Proper tests that would get run by pytest and your CI/CD pipeline
- examples and mocks (harnesses) :: Well-behaved "context" scripts for
     prototyping, bugfixing, and showcasing how to accomplish very
     specific tasks.
- troubleshooting :: Misbehaved "context" scripts for broad domain
     problem solving. This is more oriented towards improving the
     operation of the repo tooling, how installations are failing, how
     builds are failing etc.
- scrapyard :: If you feel too much apprehension in burninating your
               lovely prototype or script park it here to rust in
               peace.


The harnesses, troubleshooting, and scrapyard folders should be
flat. That means don't nest directories for categorization, instead
put it in the file name. If you can use an org-mode file to contain
explanations, instructions, or multiple code blocks, please do so. It
helps immensely to have all of the necessary context in one artifact
if possible. If you absolutely must have more than one file (if you
have config files small inputs that must be read from the program to
operate etc.) for the unit go ahead and make a directory.

For data that should be stored in git LFS (large file storage) please
put them in either:

- lfs-data :: for the automatic 'tests' data. These are relied on
              being available to run the tests and should be kept
              organized and clean.
- lfs-misc :: for all the files that are used by the harnesses,
              troubleshooting, and scrapyard. Although try not to
              store large data at all for these things, or when it is
              no longer need it remove them from the repo and untrack
              with LFS.

**** Tests

This is the thing that most developers think of. Basically we run
pytests and you can write tests like you would for that, so go read
that documentation.

I do offer some insights into our focus however. Because we do not
have unit testing we focus more on building up a collection of useful
fixtures, which build on each other. This is to approximate some
integration testing where all the components must work to even get the
end product.

The favorite test system is the Lennard-Jones (lj) pair, for which we
can build a system with no input files, along with a dependency on
openmmtools.

The integration tests for this basically amount to just importing the
fixtures. If the fixture generation part works, then we just pass the
test.

We also have a series of special test cases which are tagged as
'interactive' (which also appears in the test name).

**** Harnesses

These are scripts and code blocks that build up a mock system or
"harness" to allow interactive prototyping. These should share code
through copy-paste. be independent, and never assumed to work how you
think. Usually you will use one of these for developing the feature or
component. These should not have a module structure and should be
copy-pastable into an IPython session or notebook and run using the
dev virtual environment. If you have explanations or other
instructions please put them into an org-mode document along with the
the script in a code block which can be copy-pasted or tangled.

Because, wepy doesn't have config files and other such things you
should be able to put everything into a single python code block. This
is kind of the litmus test for whether it belongs in the harnesses or
not. If you have to set new virtual envs or do reinstallations
etc. your problem is in troubleshooting.  The only exception is if you
are prototyping something that brings in a new library, which should
be rare for core wepy. If this is the case, consider that you should
be doing this in a separate repo. The idea is that these code blocks
should be runnable version to version and the only thing that might
break is the API calls.

**** Troubleshooting

Scripts and code block/prose for specific contexts that used for
problem solving. Ideally, once a troubleshooting problem is fixed
there should be no need for the file, so go ahead and remove it,
unless you suspect the problem will rear its ugly head again. These
contexts, typically are for pathological cases and as such may involve
tweaking environmental knobs like virtual envs, package versions, OS
env variables, etc. So you probably should be writing an org mode file
(or I guess markdown; whatever floats your boat) that is very detailed
in the process providing copy-pasted outputs from your terminal
etc. Try to include a date or commit that you are working from so
future devs know what to clean out based on how old it is. If that is
you don't clean up your own mess.

**** Scrapyard

Really this is just a dumping ground for half-baked, forgotten, or
dead end things that never went anywhere. They can be prototypes,
deprecated modules, harnesses, troubleshooting scripts, anything. We
make no effort to organize anything in here at all. The idea is that
if you have this nagging feeling in the back of your mind that you
really shouldn't completely delete that thing and lose it forever. Of
course if it is in the git history it is safe (sort of), but no one
goes digging in git history for parts and pieces, its more useful for
merging branches and recovering when things go horribly wrong.

That said don't be offended if your old scrap pieces get
deleted. There are no naming conentions and there never should be
here. If you have "picked parts" they probably should go in harnesses.



** Contributing

TBD


* Architecture

** Record Groups

The protocol by which non-trajectory data is given by the resampler
and boundary conditions (BC) is unified that makes it simpler to save in
formats like HDF5.

The resampler and BC both have multiple record groups:
- resampler
  - resampling
  - resampler
- BC
  - warping
  - progress
  - boundary conditions

A record group can be thought of as a single table in a relational
database. Each record group corresponds to a class of events that
occur and each record in a record group corresponds to one event.

Record groups can be *continual* or *sporadic*.

A continual record is recorded once per cycle. A continual record
reports on the event of a cycle.

A sporadic record can be reported 0 or many times per cycle and
responds to the event determined by the record group.

- continual
  - progress
- sporadic
  - resampler
  - resampling
  - warping
  - boundary conditions

As you can see currently most records are sporadic. This distinction
is really only used internally within the ~WepyHDF5~ class to
distinguish how it stores them, but this distinction is useful in data
analysis as well.

**** Resampling Records
The ='resampling'= records are probably the most important records for
~wepy~ because they are what records the cloning and merging of
walkers.

Without the ='resampling'= your ~wepy~ simulation would have been wasted
since you no longer will know the history of any given frame. You will
just have a bag full of unconnected pictures.

Records for ='resampling'= happen for each "assignment" event of a
walker during resampling, this minimally should contain two fields:
='decision_id'= and ='target_idxs'=.

The ='decision_id'= is an integer corresponding to an enumeration of the
possible decisions that can be made as to the fate of the walker
during resampling. While technically these decisions are also modular
it is likely that 99.9% of all users will use the ~CloneMergeDecision~.

Detailed knowledge of this formalism is not usually needed in the
practice of writing resamplers that behave well, which is another
topic, and the next few paragraphs can be safely skipped.

The enumerated decisions in this are:

| =NOTHING=    | 1 |
| =CLONE=      | 2 |
| =SQUASH=     | 3 |
| =KEEP_MERGE= | 4 |


The =NOTHING= decision means don't clone or merge this walker.

=CLONE= means clone this walker.

=SQUASH= and =KEEP_MERGE= are related in that both involve merging.

A single merge includes a set of walkers that will be merged together,
there must be at least 2 such walkers in this "merge group".

From the merge group only a single /state/ will be preserved in the
single resulting walker, while the weight of the final walker will be
the sum of all those walkers.

The state of the final walker will be drawn from the set of walkers in
the merge group based on the behavior of the resampler (usually a
choice weighted by their weights), but will always be identical to one
of the walkers. The walker with the chosen state is the =KEEP_MERGE=
walker. The rest are the =SQUASH= walkers.

The second field, ='target_idxs'=, actually determines which walkers
will be merged with what other walkers, and is a tuple of integers
indicating the location, or slot.

A 'slot' is simply an available position in the lineup of walkers that
will be simulated in a single cycle of WE. The number of slots is the
number of walkers that will be simulated in the next cycle.

As an aside: In general the number of walkers used in a WE simulation
is not specified (other than there needs to be more than 1). You can
have a constant number of walkers, or a dynamic one with the number
fluctuating during the simulation.

If you have too small a number of walkers then you will have a
relatively sparse coverage of the sample space.

If you have too many the cycle throughput will be very slow.

Additionally, simulations run with GPUs will want to have a number of
walkers each cycle that is a multiple of the number of GPUs or a
number of the GPUs will be lying idle when the task queue of running
walker runner segments is depleted.

So typically there is some constraint on the the number of slots
available in the next WE cycle. The constraint is decided on and
enforced by the resampler. So if there is a mismatch in the resampling
records and the walkers produced the ~wepy~ simulation manager will
not complain.

WARNING: Currently the ~WepyHDF5~ storage backend and reporter do not
support dynamic numbers of simulations. While technically the
none of the other code has any problem with this.

The ='target_idxs'= value for =NOTHING= and =KEEP_MERGE= is a 1-tuple of
the integer index of slot where the resultant walker will be placed.

The ='target_idxs'= for =CLONE= is an n-tuple of integer indices of
slots where n is the number of children of the clone and n must be at
least 2 (or it would've been a =NOTHING=).

The ='target_idxs'= of =SQUASH= is also a 1-tuple like =NOTHING= except
since a =SQUASH= has no child it indicates the =KEEP_MERGE= walker
that it's weight is added to. Note that this slot index is the slot
index that the =KEEP_MERGE= record itself specifies and not the slot
the =KEEP_MERGE= walker previously occupied (as that index is of no
consequence to the current collection of walkers).

Thus a =KEEP_MERGE= walker defines a single merge group, and the
members of that merge group are given by which =SQUASH= targets.


Critically, the ='step_idx'= and ='walker_idx'= (slot index of walker in
last cycle) fields should also be supplied so that the lineage
histories can be generated.

In addition to the Decision class record fields any other amount of
data can be attached to these records to report on a resampling event.

For example in the WExplore resampler the region the walker was
assigned to is also given.


**** Warping Records

The next most important record is the warping records.

These are of course only relevant if you are using boundary
conditions, but among the three BC these are the principal object.

Warping records determine the action that was taken on a walker after
it met the criteria for a boundary condition event.

Minimally it should specify the ='walker_idx'= that was acted on, and if
any warping event can be discontinuous the 'weight' of it so this can
be accounted for in analysis.

The rest of the specification for boundary conditions does not have a
protocol similar to the one for cloning and merging records and is
left up to the developer of the class to decide.

For simple boundary conditions where there is only one result an
additional field is not even necesary.

The colored trajectories examples provides a possible example. In this
case you could have a field called ='color'= which is the new "color" of
the walker which indicates the last boundary it crossed and could be a
string or an integer enumeration.

**** Boundary Condition Records

This and all the other record groups are really optional.

A single boundary condition record reports on the event of a change in
the state of the boundary condition object.

For example if the cutoff value for a ligand unbinding boundary
condition changes during a simulation.

**** Resampler Records

These records report on events changing of the state of the resampler.

For example in WExplore a single record is generated every time a new
region/image is defined giving details on the values that triggered
this event as well as the image that was created.

This interpretation is semantically useful but in practice this
reporter could also report on collective attributes of the walkers,
such as all-to-all distances or histograms of the current batch of
walkers.

Its up to the writer of the resampler to decide.

**** Progress Records

Progress records are provided mainly as a convenience to get on-line
data analysis of walkers during a simulation.

For instance in ligand unbinding the progress may be the distance to
the cutoff, or RMSD to the original state.

While the active observer may note that these calculations may also
have been implemented in a reporter as well.

There are a few tradeoffs for that approach though.

One, the value may have already been calculated in the process of
evaluating walkers for warping and double calculation is potentially
unacceptably wasteful (although one might imagine complex systems
where reporters perform their actions asynchronously to the flow of
the simulation manager moving onto new cycles).

Second, the flow of data will be forked. For example when using the
~WepyHDF5Reporter~ all the data it will report on is assumed to be
contained in records returned by the runner, resampler, and boundary
conditions and can't know of another reporter. Nor is it easy nor wise
to have two reporters acting on the same database.

Perhaps such analysis could be implemented as analysis submodules in
the ~WepyHDF5Reporter~ to keep a single stream of data, if you think
that way go ahead and make a pull request.

*** Specifying Record Group Fields

Each record group should have three class constants defined for it.

This is strictly not necessary from the perspective of either the
simulation manager or the primary consumer of these records, the
~WepyHDF5Reporter~, but is a very good practice as it will help catch
bugs and will clarify the results your BC or resampler will produce
for those inspecting them.

The three definitions are:
- field names
- shapes
- dtypes


Each should be defined as a class constant prefixed by the name of the
record group followed by the definition type, for example the
resampling record group of WExplore looks like this:

#+BEGIN_SRC python
    DECISION = MultiCloneMergeDecision
    RESAMPLING_FIELDS = DECISION.FIELDS + ('step_idx', 'walker_idx', 'region_assignment',)
    RESAMPLING_SHAPES = DECISION.SHAPES + ((1,), (1,), Ellipsis,)
    RESAMPLING_DTYPES = DECISION.DTYPES + (np.int, np.int, np.int,)
#+END_SRC

For the "fields" this is the name of the field and should be a
string. In the example we are using fields defined from the
~MultiCloneMergeDecision~ class.

The shapes are the expected shapes of a single element of the
field. Three types of values are accepted here:

A. A tuple of ints that specify the shape of the field element
   array.

B. Ellipsis, indicating that the field is variable length and
   limited to being a rank one array (e.g. =(3,)= or =(1,)=).

C. None, indicating that the first instance of this field will not
   be known until runtime. Any field that is returned by a record
   producing method will automatically interpreted as None if not
   specified here.

Note that the shapes must be tuple and not simple integers for rank-1
arrays.

It is suggested that if possible use option A. Option B will use a
special datatype in HDF5 for variable length datasets that can only be
1 dimensional, in addition to being much less efficient to store.

Option C is not advisable but is there because I know people will be
lazy and not want to define all these things. By defining things ahead
of time you will reduce errors by catching differences in what you
expect a field to look like and what you actually receive at runtime.

If you are actually saving the wrong thing and don't specify the shape
and dtype then you may run weeks of simulations and never realize you
never saved the right thing there.


The dtypes have similar options but there is no Ellipsis option.

Each non-None dtype should be a numpy dtype object. This is necessary
for serializing the datatype to the HDF5 (using the
~numpy.dtype.descr~ attribute).

*** Record Fields

One additional class constant can be defined to make analysis in the
future easier.

When accessing records from a ~WepyHDF5~ object you can automatically
generate ~pandas.DataFrames~ from the records, which will select from
a subset of the fields for a record group. This is because large
arrays don't fit well into tables!

So you can define a subset of fields to be used as a nice "table"
report that could be serialized to CSV. For instance in WExplore's
resampler record group we leave out the multidimensional ='image'=
field:

#+BEGIN_SRC python
    RESAMPLER_FIELDS = ('branching_level', 'distance', 'new_leaf_id', 'image')
    RESAMPLER_SHAPES = ((1,), (1,), Ellipsis, Ellipsis)
    RESAMPLER_DTYPES = (np.int, np.float, np.int, None)

    # fields that can be used for a table like representation
    RESAMPLER_RECORD_FIELDS = ('branching_level', 'distance', 'new_leaf_id')
#+END_SRC


Again, its not necessary, but its there to use.
