* Simulation manager architecture

** TODO [#B] allow for resampler to get information on the whole tree of data :feature:
* HDF5
** TODO [#B] check file is correct

I noticed that constructing a WepyHDF5 object from a TrajHDF5 file
there is no complaint. There should be.

** TODO [#A] sparse velocities etc. across trajectory                  :core:

How to actually implement this?

In the end we want an array to be smaller than the max size array for
a trajectory which has the length equal to the number of cycles.

That means that we will store only a subset of the values.

In HDF5 we cannot store nans so we just store the values we are
given. Then in another array we need to store the cycle idxs for which
they are a part of.

I was considering using NaNs to deal with the missing values but I
think NaNs are used for when computations return bad values. So I will
scrap thata. Usually people associate nans with bad things and we
aren't doing bad things.

So I will combine the cycle idxs for the given data and return a
masked array.

However I think I will use nans when you return the actual masked
array so that if there is a mistake or someone mucks with the array it
will actually point to something being wrong.

Okay so after looking at masked arrays, I think it is a good
idea. Becasue for one using nan as the value under the mask could also
be a value in the valid array, due to an error or something so having
the mask explicitly defined will raise no ambiguities.

To implement the sparse idxs I will have a 'hidden' group which is the
sparse indexes, `_sparse_idxs`, which will have a structure the same
as the main traj group and the datasets will just be the cycles for
which the data for each is assigned.

To implement this as an API however I will have to make methods which
access it and create the array with the fill values and the mask as a
masked array. 

Then this will have to be used in different situations appropriately.

What are the situations where this will complicate things??
- computing values

Ok so what are the actual methods that need to be modified to add this?

Ok well lets consider the function signature I was developing before:


Here is the extended discussion I had with myself on this topic:

#+BEGIN_QUOTE
    Option 1:

    I could make the field a group instead of a dataset.  In that group
    you would have the sparse data in a dataset and another dataset for
    the indices they correspond to.

    Pros:
    - index and data are together
    - cleaner top-level hierarchy

    Cons:
    - more difficult to implement because we have to separate handling of
      these types and added to the compound types this becomes pretty tricky

    Option 2:

    Other option is just to hold the indices somewhere else at the top
    level and then have the keys the same for the sparse datasets.

    Pros:
    - easier to implement
    - possibly less bug prone


    Cons:
    - for someone browsing the datastructure it is more difficult to
      figure out what is going on, i.e. less semantic
    - more complex implementation of API


    I am leaning more on the side of Option 1. That is because it seems
    more proper. Because then datasets are atomic and don't have
    far-reaching dependencies across the hierarchy. If a dataset has
    value-added complexity it is it's burden to implement it and not the
    burden of somewhere else in the data structure to keep track of it.

    It will be a little bit more complex to handle those types and would
    involve, flags at the top level.

    That is okay once it is debugged, but something may crop up.

    In Option 2 lets say someone doesn't like my API and just wants to use
    the HDF5 file directly. Then it might not be obviouis where that stuff
    is.

    And really the difference between 1 and 2 is just some implementation
    of the API.

    SO really I should consider the API.

    What are the use cases for TrajHDF5

    One use case is just dumping a trajectory from memory into the file.

    In this case to have sparse data you would likely have multiple arrays
    so you might do something like

    #+BEGIN_SRC python
      TrajHDF5(positions, velocities=sparse_velocities, velocities_idxs=velocities_idxs)
    #+END_SRC


    Where if velocities receives an array smaller than positions it knows
    it is sparse and will raise an error if it also doesn't get
    velocities_idxs, or if it gets velocities idxs it will assum
    velocities is sparse.

    The other method is the reporter where you are adding data to the HDF5
    incrementally.

    In my code now I have an `append` function which really in relation to
    python analogies is an `extend`.

    So lets consider the append first because it is similar.

    You might add a single frame of coordinates this way without velocities.
    #+BEGIN_SRC python
      traj_h5.append(positions)
    #+END_SRC


    then in a subsequent step you add velocities.


    #+BEGIN_SRC python
      traj_h5.append(positions, velocities=velocities)
    #+END_SRC

    Then it knows what index to mark down for the velocities.

    For the extend it is the same way except we will probably want to
    assume that when we add a block of values that they are
    homogeneous. That is we don't extend our trajectory with 10 frames of
    positions and 5 frames of trajectories at once.

    I.e. extends are atomic in type.

    So you would have to break it up into multiple extends.

    Now that is nice but resizing HDF5 datasets can be costly in time. So
    we might want to add an advanced option to allow for this.


    This would look like the constructor:
    #+BEGIN_SRC python
      traj_h5.extend(positions, velocities=sparse_velocities, velocities_idxs=velocities_idxs)
    #+END_SRC


    In this case the `velocities_idxs` would be the indices relative to
    the positions passed in.

    THey would then have to be converted to the indices in the hdf5 positions then added.

    Ok I think that is the way to go for the API then.


    Ok I also think I will go with Option 1 then because that is what my
    gut wants to do.

#+END_QUOTE


I want to further think about this. How do we create a sparse dataset?
Should it be defined at the initialization of the run?

Pro:
- don't have to handle and restructure the HDF5 during runtime
  - faster
  - less code
  - less complicated
  - less errors
- no strange behavior
- explicit
- can be wrapped in general use settings
  - e.g. you select a compliance level, and then set frequency of
    saves in an interface then it does the rest

Con:
- have to specify up front
  - less user-friendly
  - lots of kwargs in run intialization

I think its better to specify up front.

But we could use dynamic sparsity as a fault tolerance mechanism....

Well for that we could just make all things sparse and then leave it
up to the user to make sure they aren't skipping positions etc.

That could be a FAULT_TOLERANCE sparsity settings package.

Ok I will make it so you have to specify it up front.

How to handle compound sparse data. I think that observables should be
default sparse and parameters by default not sparse. Or For simplicity
they both are default dense (not sparse).

Well in any case how should this be specified?

Ok finished for the constructor of TrajHDF5 for sparse fields given in
the constructor.

Need to do this for the extend method now though.

Then should analyze the get methods and accomodate.

Ok I got the compound fields working as well.

Well since I don't have any specific methods for getting data it isn't
a problem. I suppose that was because there wasn't anything particular
about getting the data before, so that new methods are warranted here
because of sparse trajectories.

I will do the extend method now.

Ok the issue here is that it is using outdated methods for extend: it
checks to see if the value is a group for if it is compound or
not. This will be a problem for sparse data.

It is probably just okay to do a rewrite and plunder snippets where
they can be useful.

This will involve:
- [X] update_sparse_flags function to be called when a read file object is
  made that can detect based on structure of the field (or perhaps
  flags in the file itself)
- [ ] usage of compound group flags to get wether a field is compound
  or not. DONT need this because we know which ones are compound based
  on the CONSTANT flags for compound groups.

Extend function finished.

Now I need to decide on the access methods. This will involve getting
data out as a masked array from sparse datasets.

This will probably be a function like ~traj_h5.get_field(field_name)~ or
~traj_h5.field(field_name)~ that returns an array.

Also would like to have access methods so that you can directly access
them as properties like ~traj_h5.positions~ and ~traj_h5.velocities~.

*** DONE TrajHDF5 sparse trajectories
**** DONE [#A] TrajHDF5 get method for sparse trajectories             :core:

Analyzing behavior and appropriateness of the masked array for this
purpose.

One thing I have noticed that is unfavorable is that when you perform
an operation with a validity domain that can return invalid values
(i.e. log and divide) that instead of returning a masked array with
the same mask it will mask the invalid values. This is bad behavior
for calculating things like energies where I want to know where it
fails and use a fill value for those. In this implementation it
convolutes what is actually being masked and unmasked but invalid
values.

This is how it is implemented though.

**** DONE [#A] TrajHDF5 ensure that all fields have correct lengths    :core:

This means having the number of frames known and not adding different
size datasets on construction (without sparsity), and not allowing
different size datasets at all on extends.

**** DONE [#A] for the TrajHDF5 constructor

**** DONE [#A] initialize sparse_fields in constructor if no data given :core:
After correcting some more core issues I now have the problem of
specificying a field as sparse without giving any data in the
constructor.

Because the init function sub-constructor is doing this how will I
tell it to do that?

I could just initialize all the fields from the start. This would work
for velocities or main fields but not for observables and other compound fields.

I don't have a function to initialize a dataset as empty but maybe I should.

This requires knowing both the shape and the dtype... annoying..

Is it worth it??

For velocities etc. it is okay because it will be the same as the
positions or can be defined in the class or object or module

For other things (like in observables) it cannot and in other places
we have made this something that can be defined or not depending on if
it is important or not. For instance in auxiliary data.

Here what to do?

I think it should at least be possible to create the dataset when you
first add (extend) to the data if it doesn't exist but has been
defined as sparse.

Key things should be initialized at the start to make sure they aren't
written in a bad way. i.e. velocities. Okay define them in the module.

Number of spatial dimensions is an object variable defaulting to the
module one.

So I guess that means the velocity shape should be an object value.

other ones:
- time : module constant
- box_vectors : object constant with module default
- velocities : derived from positions
- forces : derived from positions
- kinetic_energy : module constant
- potential_energy : module constant
- box_volume : module constant
- parameters : not defined in module
- parameter_derivatives : not defined in module
- observables : not defined in module

well actually we really need the rank of the feature to initialize the
dataset shape. We do need the actual dimensions for the maxshape which
is important but ranks can all be given (except for compounds in the
module level).

Maybe we don't need the ranks because we can just set them from
changing the values of the shape tuples.

I will keep them though it will still be useful.

Forgot about dtypes.

How to handle those.

Well I kind of want to hardcode those except that I am thinking about
test kinds of systems like the randomwalk example.

This would have discrete values for positions, time, etc.

This requires some thinking because the 'box_vectors' attribute would
not be really a consideration.

Maybe I could do a similar thing to mastic and have the constants for
MD in another file which is used as default but not necessarily.

I don't want to go down to far down this rabbit hole and it is
probably better to have an example in front of me to design against
than just guess.

Again this is also just for initializing fields without any data to go
with them. So what are the cases this would actually happen?

- Sparse fields for non-MD data.
- I suppose if we want to give the option to initialize fields 

The latter would require a set of methods to do so. Maybe this is a
good idea to write because I could use it here.

Something like:

#+BEGIN_SRC python
  def _init_field(self, field_path, feature_shape, dtype):
      # get the group to put the field under and the name to use
      grp, field_name = self.get_path_grp(field_path)

      grp.create_dataset(field_name, (0, *[0 for i in feature_shape]), dtype=dtype,
                         maxshape=(None, *feature_shape))
#+END_SRC

And then we would just have to manage how to feature_shape and dtype
are passed to this.

The rank constants might be useful but only in determining the shape
perhaps.

THere is definitely a lot of simplification that could be done with
the compound groups with the invention of the pattern I used in the
`_get_field_path_grp` method.

#+BEGIN_SRC python
      def _get_field_path_grp(self, field_path):
          """Given a field path for the trajectory returns the group the field's
          dataset goes in and the key for the field name in that group.

          The field path for a simple field is just the name of the
          field and for a compound field it is the compound field group
          name with the subfield separated by a '/' like
          'observables/observable1' where 'observables' is the compound
          field group and 'observable1' is the subfield name.

          """

          # check if it is compound
          if '/' in field_path:
              # split it
              grp_name, field_name = field_path.split('/')
              # get the hdf5 group
              grp = self.h5[grp]
          # its simple so just return the root group and the original path
          else:
              grp = self.h5
              field_name = field_path

          return grp, field_name

#+END_SRC

This means you don't have to have a special case in every method just
for compound groups. You simply pass around the values for the very
simple paths and call this method and then do what you need to do on
the returned hdf5 group instead of iterating again through a
sub-dictionary just for the compound groups.

I wonder if there is a Type in strict type languages for this kind of
behavior where the key can be nested itself, beyond a string convention.

Okay I got a good framework that allows passing in shapes and dtypes
but will revert to the default if not given.

It doesn't allow you to do default plus other stuff. Nuts to people
like that. Just do the whole thing yourself then.

But it actually needs to initialize these as sparse datasets not normal ones.

Right now I wrote the init function to do just a normal dataset which
receives a field path so it works for compound fields.

How to get it to recognize sparse fields? It should be an
attribute. It could also go on flags passed in but I will try the
attribute first since it is a hidden class method.

Ok the sparse_field method has two sub-methods to distinguish.

Okay Now I need to make sure that the anonymous fields that come in at
runtime are also accomodated.

Testing my previous changes and now we have the problem that when we
want to actually add data to the thing the shape is all wrong since it
is empty and only has the same rank.

So we need to add to the extend function the case where the dataset is
empty.

Is that right?? I think so. The 'add' functions are for when you want
to set it all at once or want to initialize it with a big chunk of data.

Perhaps that could be integrated into an initialization scheme
together with the empty initializers. I think that the 'add' functions
really are the ones that should change and not the initialization
stuff I just wrote. Plus the initialization stuff is using more
elegant patterns that those should copy anyways :P

So lets just make a case in the extend (and if append ever exists) to
set to resize to the maxshape then set the data.

That worked out easily enough.

Moving ont o handle things passed in at runtime that are not
initialized from settings given at the beginning.

had to add some checking in the init sparse field function.

I check to see if it is compound and make the group for the subfield
if it doesn't exist.

Then if it is a runtime defined sparse field value (no initialization)
we don't try to initialize it here.

Actually taht is stupid. We should just not call this function if that
is the case and handle it in the function that is calling this one.

Fixed some things with initializing groups for compound fields.

Now I am at an error which I can move forward with initializing at run
time for a compound sparse field which is not specified at the
beginning.

That was easy!! Just added an if that checks to see if it was
initialized and then uses the init function to initialize it.

Then normal extend can be used.

Is that it?? what other possibilities are there?

Fixed get_field. Was not working with compound paths.

**** DONE [#A] Rename simple referring to non-sparse contiguous to be more literate
and not collide with the usage with simple as meaning not-compound.


*** TODO [#A] WepyHDF5 sparse trajectories
**** DONE [#A] WepyHDF5 ensure that all fields have correct lengths    :core:
**** TODO [#A] WepyHDF5 initialize sparse_fields in constructor if no data given
**** TODO [#A] WepyHDF5 get method for sparse trajectories             :core:



** TODO [#A] restarting simulations, multiple runs                     :core:
** TODO [#A] implement SWMR                                         :feature:

** TODO [#A] fix compute observable to write feature vector style      :core:





** TODO [#C] use h5py variable length datasets instead of my solution :feature:backend:

Didn't know this was a feature of h5py and am curious to see how this
is implemented underneath and whether it is an hdf5 standard thing.

H5py is not the only library we want to be read this data from.

** TODO [#C] use h5py enumeration type instead of my solution :feature:backend:

** TODO [#B] implement run cycle slice                  :feature:api:nazanin:

** TODO [#B] implement run cycle map function           :feature:nazanin:api:

** TODO [#B] implement run cycle compute observables    :feature:nazanin:api:
** TODO [#B] concat function                                    :feature:api:

I want to have a concat function similar to other major libraries that
puts runs from different simulations together. The specifications I
want it to have are:

- options for inplace and copying
  - inplace on a 'master' file object, probably the first in the list passed.
  - another option (True by default) which deletes the members of the
    concat after a successful concatenation
  - make a copy of the new file and leave all the others untouched

** TODO [#B] full slice across datasets in TrajHDF5             :feature:api:

get all values for a collection of indices, with fancy slicing

Call it a cycle cross section.

Should be a function for each field of a run to get the cycle data:
- cycle_resampling(run_idx, cycle_idxs)
- cycle_boundary_conditions(run_idx, cycle_idxs)
- cycle_warping(run_idx, cycle_idxs)
- cycle_trajectories(run_idx, cycle_idxs)
- cycle_cross_section(run_idx, cycle_idxs, fields=['trajectories', 'resampling',
                                                   'boundary_conditions', 'warping'])
  - which calls the other functions based on what they are.



** TODO [#B] allow for arbitrary number of frames to be saved in HDF5 traj part :core:
** TODO [#B] allow for passing in of real np.dtypes to resampling records :core:api:

special handling for the variable length "tokens"

** TODO [#B] original WExplore algorithm                :feature:application:
** TODO [#B] add records for the boundary conditions               :core:api:
This needs to be implemented in the WepyHDF5 and in the actual
boundary conditions class.

** TODO [#C] implement chunking strategies                      :feature:api:

- [ ] protein, ligand, solvent
- [ ] ligand, binding-site

** TODO [#C] compliance infrastructure                          :feature:api:

** TODO [#C] only accept Quantity type objects that match/convert units :feature:api:

This will require choosing a unit library:
- simtk.units
- pint

** TODO [#C] HDF5 topology                                    :core:topology:

** TODO [#C] simulation reproducibility metadata                :feature:api:

** TODO [#C] traj object for trajs in WepyHDF5                  :feature:api:

This would have the same API as the TrajHDF5 object.

** TODO [#C] HDF5 topology                                :core:topology:api:

This needs to be developed.
- JSON represenation also capable to be converted to and from

** TODO [#C] add support for trajectory total ordering          :feature:api:

That means a single unique positive integer index for every trajectory in the whole file.

Support this as an trajectory selector in the iter_trajs.


** TODO [#B] add records for the boundary conditions                   :core:
This needs to be implemented in the WepyHDF5 and in the actual
boundary conditions class.

** TODO [#C] save weights on export_traj to TrajHDF5                :feature:

Save them in the observables.

Do we save them automatically?
as an option?
- [X] Or must be done manually?
