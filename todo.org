* Simulation manager architecture

** TODO [#B] allow for resampler to get information on the whole tree of data :feature:
* HDF5
** TODO [#B] check file is correct

I noticed that constructing a WepyHDF5 object from a TrajHDF5 file
there is no complaint. There should be.


** TODO [#A] sparse velocities etc. across trajectory                  :core:

How to actually implement this?

In the end we want an array to be smaller than the max size array for
a trajectory which has the length equal to the number of cycles.

That means that we will store only a subset of the values.

In HDF5 we cannot store nans so we just store the values we are
given. Then in another array we need to store the cycle idxs for which
they are a part of.

I was considering using NaNs to deal with the missing values but I
think NaNs are used for when computations return bad values. So I will
scrap thata. Usually people associate nans with bad things and we
aren't doing bad things.

So I will combine the cycle idxs for the given data and return a
masked array.

However I think I will use nans when you return the actual masked
array so that if there is a mistake or someone mucks with the array it
will actually point to something being wrong.

Okay so after looking at masked arrays, I think it is a good
idea. Becasue for one using nan as the value under the mask could also
be a value in the valid array, due to an error or something so having
the mask explicitly defined will raise no ambiguities.

To implement the sparse idxs I will have a 'hidden' group which is the
sparse indexes, `_sparse_idxs`, which will have a structure the same
as the main traj group and the datasets will just be the cycles for
which the data for each is assigned.

To implement this as an API however I will have to make methods which
access it and create the array with the fill values and the mask as a
masked array. 

Then this will have to be used in different situations appropriately.

What are the situations where this will complicate things??
- computing values

Ok so what are the actual methods that need to be modified to add this?

Ok well lets consider the function signature I was developing before:


Here is the extended discussion I had with myself on this topic:

#+BEGIN_QUOTE
    Option 1:

    I could make the field a group instead of a dataset.  In that group
    you would have the sparse data in a dataset and another dataset for
    the indices they correspond to.

    Pros:
    - index and data are together
    - cleaner top-level hierarchy

    Cons:
    - more difficult to implement because we have to separate handling of
      these types and added to the compound types this becomes pretty tricky

    Option 2:

    Other option is just to hold the indices somewhere else at the top
    level and then have the keys the same for the sparse datasets.

    Pros:
    - easier to implement
    - possibly less bug prone


    Cons:
    - for someone browsing the datastructure it is more difficult to
      figure out what is going on, i.e. less semantic
    - more complex implementation of API


    I am leaning more on the side of Option 1. That is because it seems
    more proper. Because then datasets are atomic and don't have
    far-reaching dependencies across the hierarchy. If a dataset has
    value-added complexity it is it's burden to implement it and not the
    burden of somewhere else in the data structure to keep track of it.

    It will be a little bit more complex to handle those types and would
    involve, flags at the top level.

    That is okay once it is debugged, but something may crop up.

    In Option 2 lets say someone doesn't like my API and just wants to use
    the HDF5 file directly. Then it might not be obviouis where that stuff
    is.

    And really the difference between 1 and 2 is just some implementation
    of the API.

    SO really I should consider the API.

    What are the use cases for TrajHDF5

    One use case is just dumping a trajectory from memory into the file.

    In this case to have sparse data you would likely have multiple arrays
    so you might do something like

    #+BEGIN_SRC python
      TrajHDF5(positions, velocities=sparse_velocities, velocities_idxs=velocities_idxs)
    #+END_SRC


    Where if velocities receives an array smaller than positions it knows
    it is sparse and will raise an error if it also doesn't get
    velocities_idxs, or if it gets velocities idxs it will assum
    velocities is sparse.

    The other method is the reporter where you are adding data to the HDF5
    incrementally.

    In my code now I have an `append` function which really in relation to
    python analogies is an `extend`.

    So lets consider the append first because it is similar.

    You might add a single frame of coordinates this way without velocities.
    #+BEGIN_SRC python
      traj_h5.append(positions)
    #+END_SRC


    then in a subsequent step you add velocities.


    #+BEGIN_SRC python
      traj_h5.append(positions, velocities=velocities)
    #+END_SRC

    Then it knows what index to mark down for the velocities.

    For the extend it is the same way except we will probably want to
    assume that when we add a block of values that they are
    homogeneous. That is we don't extend our trajectory with 10 frames of
    positions and 5 frames of trajectories at once.

    I.e. extends are atomic in type.

    So you would have to break it up into multiple extends.

    Now that is nice but resizing HDF5 datasets can be costly in time. So
    we might want to add an advanced option to allow for this.


    This would look like the constructor:
    #+BEGIN_SRC python
      traj_h5.extend(positions, velocities=sparse_velocities, velocities_idxs=velocities_idxs)
    #+END_SRC


    In this case the `velocities_idxs` would be the indices relative to
    the positions passed in.

    THey would then have to be converted to the indices in the hdf5 positions then added.

    Ok I think that is the way to go for the API then.


    Ok I also think I will go with Option 1 then because that is what my
    gut wants to do.

#+END_QUOTE


I want to further think about this. How do we create a sparse dataset?
Should it be defined at the initialization of the run?

Pro:
- don't have to handle and restructure the HDF5 during runtime
  - faster
  - less code
  - less complicated
  - less errors
- no strange behavior
- explicit
- can be wrapped in general use settings
  - e.g. you select a compliance level, and then set frequency of
    saves in an interface then it does the rest

Con:
- have to specify up front
  - less user-friendly
  - lots of kwargs in run intialization

I think its better to specify up front.

But we could use dynamic sparsity as a fault tolerance mechanism....

Well for that we could just make all things sparse and then leave it
up to the user to make sure they aren't skipping positions etc.

That could be a FAULT_TOLERANCE sparsity settings package.

Ok I will make it so you have to specify it up front.

How to handle compound sparse data. I think that observables should be
default sparse and parameters by default not sparse. Or For simplicity
they both are default dense (not sparse).

Well in any case how should this be specified?

Ok finished for the constructor of TrajHDF5 for sparse fields given in
the constructor.

Need to do this for the extend method now though.

Then should analyze the get methods and accomodate.

Ok I got the compound fields working as well.

Well since I don't have any specific methods for getting data it isn't
a problem. I suppose that was because there wasn't anything particular
about getting the data before, so that new methods are warranted here
because of sparse trajectories.

I will do the extend method now.

Ok the issue here is that it is using outdated methods for extend: it
checks to see if the value is a group for if it is compound or
not. This will be a problem for sparse data.

It is probably just okay to do a rewrite and plunder snippets where
they can be useful.

This will involve:
- [X] update_sparse_flags function to be called when a read file object is
  made that can detect based on structure of the field (or perhaps
  flags in the file itself)
- [ ] usage of compound group flags to get wether a field is compound
  or not. DONT need this because we know which ones are compound based
  on the CONSTANT flags for compound groups.

Extend function finished.

Now I need to decide on the access methods. This will involve getting
data out as a masked array from sparse datasets.

This will probably be a function like ~traj_h5.get_field(field_name)~ or
~traj_h5.field(field_name)~ that returns an array.

Also would like to have access methods so that you can directly access
them as properties like ~traj_h5.positions~ and ~traj_h5.velocities~.

*** TODO [#A] get method for sparse trajectories                       :core:

Analyzing behavior and appropriateness of the masked array for this
purpose.

One thing I have noticed that is unfavorable is that when you perform
an operation with a validity domain that can return invalid values
(i.e. log and divide) that instead of returning a masked array with
the same mask it will mask the invalid values. This is bad behavior
for calculating things like energies where I want to know where it
fails and use a fill value for those. In this implementation it
convolutes what is actually being masked and unmasked but invalid
values.

*** TODO [#A] ensure that all fields have correct lengths              :core:

This means having the number of frames known and not adding different
size datasets on construction (without sparsity), and not allowing
different size datasets at all on extends.

** TODO [#A] restarting simulations, multiple runs                     :core:
** TODO [#A] implement SWMR                                         :feature:

** TODO [#A] fix compute observable to write feature vector style      :core:





** TODO [#C] use h5py variable length datasets instead of my solution :feature:backend:

Didn't know this was a feature of h5py and am curious to see how this
is implemented underneath and whether it is an hdf5 standard thing.

H5py is not the only library we want to be read this data from.

** TODO [#C] use h5py enumeration type instead of my solution :feature:backend:

** TODO [#B] implement run cycle slice                  :feature:api:nazanin:

** TODO [#B] implement run cycle map function           :feature:nazanin:api:

** TODO [#B] implement run cycle compute observables    :feature:nazanin:api:
** TODO [#B] concat function                                    :feature:api:

I want to have a concat function similar to other major libraries that
puts runs from different simulations together. The specifications I
want it to have are:

- options for inplace and copying
  - inplace on a 'master' file object, probably the first in the list passed.
  - another option (True by default) which deletes the members of the
    concat after a successful concatenation
  - make a copy of the new file and leave all the others untouched

** TODO [#B] full slice across datasets in TrajHDF5             :feature:api:

get all values for a collection of indices, with fancy slicing

Call it a cycle cross section.

Should be a function for each field of a run to get the cycle data:
- cycle_resampling(run_idx, cycle_idxs)
- cycle_boundary_conditions(run_idx, cycle_idxs)
- cycle_warping(run_idx, cycle_idxs)
- cycle_trajectories(run_idx, cycle_idxs)
- cycle_cross_section(run_idx, cycle_idxs, fields=['trajectories', 'resampling',
                                                   'boundary_conditions', 'warping'])
  - which calls the other functions based on what they are.



** TODO [#B] allow for arbitrary number of frames to be saved in HDF5 traj part :core:
** TODO [#B] allow for passing in of real np.dtypes to resampling records :core:api:

special handling for the variable length "tokens"

** TODO [#B] original WExplore algorithm                :feature:application:
** TODO [#B] add records for the boundary conditions               :core:api:
This needs to be implemented in the WepyHDF5 and in the actual
boundary conditions class.

** TODO [#C] implement chunking strategies                      :feature:api:

- [ ] protein, ligand, solvent
- [ ] ligand, binding-site

** TODO [#C] compliance infrastructure                          :feature:api:

** TODO [#C] only accept Quantity type objects that match/convert units :feature:api:

This will require choosing a unit library:
- simtk.units
- pint

** TODO [#C] HDF5 topology                                    :core:topology:

** TODO [#C] simulation reproducibility metadata                :feature:api:

** TODO [#C] traj object for trajs in WepyHDF5                  :feature:api:

This would have the same API as the TrajHDF5 object.

** TODO [#C] HDF5 topology                                :core:topology:api:

This needs to be developed.
- JSON represenation also capable to be converted to and from

** TODO [#C] add support for trajectory total ordering          :feature:api:

That means a single unique positive integer index for every trajectory in the whole file.

Support this as an trajectory selector in the iter_trajs.


** TODO [#B] add records for the boundary conditions                   :core:
This needs to be implemented in the WepyHDF5 and in the actual
boundary conditions class.

** TODO [#C] save weights on export_traj to TrajHDF5                :feature:

Save them in the observables.

Do we save them automatically?
as an option?
- [X] Or must be done manually?
