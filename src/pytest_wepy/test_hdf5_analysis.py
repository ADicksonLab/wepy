# Testing hdf5 functionality
#
# 1) testing if warping events are written correctly
# 2) reading in HDF5 (tests on sensible data)
# 3) compute observable
# 4) get traces

import os
import unittest

import mdtraj as mdj
import numpy as np

from wepy.analysis.contig_tree import ContigTree
from wepy.boundary_conditions.randomwalk import RandomWalkBC
from wepy.hdf5 import WepyHDF5
from wepy.resampling.decisions.clone_merge import MultiCloneMergeDecision

hdf5_filename = "test_data/rw.h5"
hdf5_filename_nores = "test_data/rw_noresampling.h5"
hdf5_filename_nowarp = "test_data/rw_nowarping.h5"
segment_length = 1
boundary_position = 5


def test_H5_warping():
    # ------------------------------------------------------------------
    # test_data/rw_noresampling.h5 holds 1D random walk data (generated by test_hdf5.py)
    # includes warping, but no resampling
    # ------
    we = WepyHDF5(hdf5_filename_nores, mode="r")
    with we:
        n_cycles = we.num_run_cycles(0)
        wr_list = we.warping_records([0])
        warp_trace = [(wr.walker_idx, wr.cycle_idx) for wr in wr_list]
        warp_pos = we.get_run_trace_fields(0, warp_trace, ["positions"])

    # all of the warp positions should be at x = boundary_position
    if len(warp_pos["positions"]) > 0:
        assert warp_pos["positions"].max() == boundary_position
        assert warp_pos["positions"].min() == boundary_position

        # make sure that the next cycle they get warped back to zero
        warp_next_trace = [
            (wr.walker_idx, wr.cycle_idx + 1)
            for wr in wr_list
            if wr.cycle_idx + 1 < n_cycles
        ]
        with we:
            warp_next_pos = we.get_run_trace_fields(0, warp_next_trace, ["positions"])

        # test if the walkers were warped back to the beginning
        assert warp_next_pos["positions"].max() <= segment_length


def test_H5_resampling():
    # ------------------------------------------------------------------
    # test_data/rw_nowarping.h5 holds 1D random walk data (generated by test_hdf5.py)
    # includes resampling, but no warping
    # ------
    we = WepyHDF5(hdf5_filename_nowarp, mode="r")
    with we:
        n_cycles = we.num_run_cycles(0)
        wr_list = we.warping_records([0])
        n_walkers = we.num_walkers(0, 0)
        all_pos = np.array(
            [we.h5[f"runs/0/trajectories/{i}/positions"] for i in range(n_walkers)]
        )
        all_wts = np.array(
            [we.h5[f"runs/0/trajectories/{i}/weights"] for i in range(n_walkers)]
        )

        rrs = we.resampling_records([0])
    #
    # find all cloning events
    #
    # MultiCloneMergeDecision: (1: Nothing; 2: Clone; 3: Squash; 4: Keep_Merge)
    #
    clone_rrs = [rr for rr in rrs if rr.decision_id == 2]
    for cr in clone_rrs:
        parent = cr.walker_idx
        targets = cr.target_idxs
        cycle_idx = cr.cycle_idx
        if cycle_idx + 1 < n_cycles:
            for target in targets:
                # test if cloned walkers have the appropriate weights
                assert (
                    all_wts[parent][cycle_idx] / len(targets)
                    == all_wts[target][cycle_idx + 1]
                )

                # test if the children are within segment_length of the parent
                assert (
                    np.sum(
                        np.abs(
                            all_pos[parent][cycle_idx] - all_pos[target][cycle_idx + 1]
                        )
                    )
                    <= segment_length
                )

    #
    # find all merging events
    #
    squash_rrs = [rr for rr in rrs if rr.decision_id == 3]
    keep_merge_rrs = [rr for rr in rrs if rr.decision_id == 4]
    for km in keep_merge_rrs:
        cycle = km.cycle_idx
        walker = km.walker_idx
        if cycle + 1 < n_cycles:
            # get all the squash records that correspond to this
            squashed_walkers = [
                sr.walker_idx
                for sr in squash_rrs
                if (sr.target_idxs[0] == walker and sr.cycle_idx == cycle)
            ] + [walker]

            # check if the sum of their weights (on this cycle) equals the km weight (on the next cycle)
            np.testing.assert_almost_equal(
                np.sum(all_wts[walker][cycle + 1]),
                np.sum(all_wts[squashed_walkers, cycle]),
                decimal=5,
            )


def test_H5_contig():
    # ------------------------------------------------------------------
    # test_data/rw.h5 holds 1D random walk data (generated by test_hdf5.py)
    # includes both warping and resampling
    # ------
    we = WepyHDF5(hdf5_filename, mode="r")
    with we:
        n_cycles = we.num_run_cycles(0)
        n_walkers = we.num_walkers(0, 0)
        all_pos = np.array(
            [we.h5[f"runs/0/trajectories/{i}/positions"] for i in range(n_walkers)]
        )
        ct = ContigTree(
            we,
            boundary_condition_class=RandomWalkBC,
            decision_class=MultiCloneMergeDecision,
        )

    sw = ct.sliding_windows(3)
    for trace in sw:
        # test if traces are from adjacent cycles
        assert trace[1][2] - trace[0][2] == 1
        assert trace[2][2] - trace[1][2] == 1

        # test if positions are adjacent
        assert (
            np.sum(
                np.abs(
                    all_pos[trace[1][1], trace[1][2]]
                    - all_pos[trace[0][1], trace[0][2]]
                )
            )
            <= 1
        )
        assert (
            np.sum(
                np.abs(
                    all_pos[trace[2][1], trace[2][2]]
                    - all_pos[trace[1][1], trace[1][2]]
                )
            )
            <= 1
        )

    final_trace = [(0, i, n_cycles - 1) for i in range(n_walkers)]
    lineages = ct.lineages(final_trace, discontinuities=True)
    for lin in lineages:
        for i in range(len(lin) - 1):
            # test if positions are adjacent
            assert (
                np.sum(
                    np.abs(
                        all_pos[lin[i][1], lin[i][2]]
                        - all_pos[lin[i + 1][1], lin[i + 1][2]]
                    )
                )
                <= 1
            )

    disc_lineages = ct.lineages(final_trace, discontinuities=False)
    for lin in disc_lineages:
        for i in range(len(lin) - 1):
            # if positions aren't adjacent, check that they correspond to warping events
            if (
                np.sum(
                    np.abs(
                        all_pos[lin[i][1], lin[i][2]]
                        - all_pos[lin[i + 1][1], lin[i + 1][2]]
                    )
                )
                > 1
            ):
                assert all_pos[lin[i][1], lin[i][2]] == boundary_position
                assert all_pos[lin[i + 1][1], lin[i + 1][2]] <= segment_length
